"""
crawler.py â€” ç„¡é™ã‚µãƒ¼ãƒå¸¸é§ãƒ¯ãƒ¼ã‚«ãƒ¼
æ¤œç´¢ã‚­ãƒ¥ãƒ¼ã¾ãŸã¯ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒªã‚¹ãƒˆã‹ã‚‰ä½œå“ã‚’é¸ã³ã€
Google News RSSç­‰ã‚’ç”¨ã„ã¦ã‚°ãƒƒã‚ºæƒ…å ±ã‚’åé›†ã—DBã«ç™»éŒ²ã—ç¶šã‘ã‚‹ã€‚
"""

import urllib.request
import urllib.parse
import xml.etree.ElementTree as ET
import time
import sqlite3
import re
import datetime
import os
import sys
try:
    import requests as req_lib
    HAS_REQUESTS = True
except ImportError:
    HAS_REQUESTS = False

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, BASE_DIR)
import database
import filter as goods_filter

def decode_google_news_url(gnews_url: str) -> str:
    """Google Newsã®é–“æ¥URLã‚’å®Ÿéš›ã®è¨˜äº‹URLã«ãƒ‡ã‚³ãƒ¼ãƒ‰ã™ã‚‹"""
    if "news.google.com" not in gnews_url:
        return gnews_url
    try:
        from googlenewsdecoder import new_decoderv1
        decoded_res = new_decoderv1(gnews_url)
        if decoded_res.get("status") and decoded_res.get("decoded_url"):
            return decoded_res["decoded_url"]
    except Exception:
        pass
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: requestsã§ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆå…ˆã‚’è¿½ã†
    if HAS_REQUESTS:
        try:
            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
            r = req_lib.get(gnews_url, headers=headers, timeout=8, allow_redirects=True)
            if "news.google.com" not in r.url:
                return r.url
        except Exception:
            pass
    return gnews_url


def fetch_google_news(query: str) -> list:
    """Google News RSS ã‹ã‚‰æŒ‡å®šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—"""
    encoded_query = urllib.parse.quote(query)
    url = f"https://news.google.com/rss/search?q={encoded_query}&hl=ja&gl=JP&ceid=JP:ja"
    req = urllib.request.Request(url, headers={'User-Agent': 'Mozilla/5.0'})

    # mediaåå‰ç©ºé–“ã®å®šç¾©
    MEDIA_NS = "http://search.yahoo.com/mrss/"

    results = []
    try:
        with urllib.request.urlopen(req, timeout=10) as response:
            xml_data = response.read()
            ET.register_namespace('media', MEDIA_NS)
            root = ET.fromstring(xml_data)
            for item in root.findall('./channel/item'):
                title = item.find('title').text if item.find('title') is not None else ""
                link = item.find('link').text if item.find('link') is not None else ""
                pubDate = item.find('pubDate').text if item.find('pubDate') is not None else ""
                source = item.find('source').text if item.find('source') is not None else "Google News"

                # dateã‚’ãƒ‘ãƒ¼ã‚¹ (RFC822å½¢å¼)
                try:
                    parts = pubDate.split()
                    if len(parts) >= 4:
                        month_map = {"Jan":"01","Feb":"02","Mar":"03","Apr":"04","May":"05","Jun":"06",
                                     "Jul":"07","Aug":"08","Sep":"09","Oct":"10","Nov":"11","Dec":"12"}
                        y = parts[3]
                        m = month_map.get(parts[2][:3], "01")
                        d = parts[1].zfill(2)
                        parsed_date = f"{y}-{m}-{d} {parts[4]}"
                    else:
                        parsed_date = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                except:
                    parsed_date = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")

                # --- ç”»åƒå–å¾—: RSSã®media:contentã‚’æœ€å„ªå…ˆ ---
                rss_image = ""
                # media:content
                mc = item.find(f'{{{MEDIA_NS}}}content')
                if mc is not None and mc.get('url'):
                    img = mc.get('url', '')
                    if img and 'google' not in img.lower():
                        rss_image = img
                # media:thumbnail
                if not rss_image:
                    mt = item.find(f'{{{MEDIA_NS}}}thumbnail')
                    if mt is not None and mt.get('url'):
                        img = mt.get('url', '')
                        if img and 'google' not in img.lower():
                            rss_image = img
                # enclosure
                if not rss_image:
                    enc = item.find('enclosure')
                    if enc is not None and enc.get('url'):
                        img = enc.get('url', '')
                        if img and 'google' not in img.lower():
                            rss_image = img

                # RSSç”»åƒãŒãªã‘ã‚Œã°å®Ÿéš›ã®è¨˜äº‹URLã‚’å–å¾—ã—ã¦og:imageã‚’æ¢ã™
                image_url = rss_image
                if not image_url and link:
                    # Google News URLã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰ã—ã¦å®Ÿéš›ã®è¨˜äº‹URLã‚’å–å¾—
                    real_url = decode_google_news_url(link)
                    # ãƒ‡ã‚³ãƒ¼ãƒ‰ã§ããŸå ´åˆã®ã¿ç”»åƒã‚’å–å¾—ï¼ˆnews.google.comã®ã¾ã¾ãªã‚‰GEã‚¢ã‚¤ã‚³ãƒ³ã«ãªã‚‹ã®ã§ã‚¹ã‚­ãƒƒãƒ—ï¼‰
                    if "news.google.com" not in real_url:
                        image_url = fetch_ogp_image(real_url)

                results.append({
                    "title": title,
                    "content": title,  # RSSã¯æœ¬æ–‡ãŒçŸ­ã„ãŸã‚ã‚¿ã‚¤ãƒˆãƒ«ã‚’ä»£ç”¨
                    "author": source,
                    "date": parsed_date,
                    "source_url": link,
                    "source_type": "Google",
                    "image_url": image_url
                })
    except Exception as e:
        print(f"[Crawler Error] RSS Fetch failed for '{query}': {e}")

    return results


def fetch_ogp_image(url: str) -> str:
    """æŒ‡å®šURLã®Pageã‹ã‚‰OGP(og:image)ã‚¿ã‚°ã®ç”»åƒURLã‚’å–å¾—ã™ã‚‹ã€‚
    ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯é †: og:image â†’ twitter:image â†’ è¨˜äº‹å†…æœ€åˆã®imgã‚¿ã‚°
    """
    # Google News / Googleã‚³ãƒ¡ãƒ³ãƒˆã®URLã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆGEã‚¢ã‚¤ã‚³ãƒ³ã«ãªã‚‹ã®ã§å¿…ãšé™¤å¤–ï¼‰
    if not url or "news.google.com" in url or url.lower().startswith("https://news.google"):
        return ""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0 Safari/537.36',
        'Accept-Language': 'ja,en;q=0.9',
    }
    original_url = url
    try:
        # Google Newsã®ä¸­é–“URL(CBMi...)ã®å ´åˆã¯ã€æœ¬ç‰©ã®è¨˜äº‹URLã«ãƒ‡ã‚³ãƒ¼ãƒ‰ã™ã‚‹
        if "news.google.com" in url:
            try:
                from googlenewsdecoder import new_decoderv1
                decoded_res = new_decoderv1(url)
                if decoded_res.get("status") and decoded_res.get("decoded_url"):
                    url = decoded_res["decoded_url"]
            except Exception:
                pass  # ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ç„¡ã—ã‚„ã‚¨ãƒ©ãƒ¼æ™‚ã¯ãã®ã¾ã¾ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯

        if HAS_REQUESTS:
            r = req_lib.get(url, headers=headers, timeout=8, allow_redirects=True)
            html = r.text
            # å®Ÿéš›ã®ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆå…ˆURLã‚’å–å¾—ï¼ˆç›¸å¯¾URLè§£æ±ºã«ä½¿ã†ï¼‰
            final_url = r.url
        else:
            req_obj = urllib.request.Request(url, headers=headers)
            with urllib.request.urlopen(req_obj, timeout=8) as resp:
                html = resp.read().decode('utf-8', errors='ignore')
                final_url = resp.url

        # ãƒ™ãƒ¼ã‚¹URLã‚’å–å¾—ï¼ˆç›¸å¯¾URLã®è§£æ±ºç”¨ï¼‰
        try:
            from urllib.parse import urlparse, urljoin
            parsed = urlparse(final_url)
            base_url = f"{parsed.scheme}://{parsed.netloc}"
        except Exception:
            base_url = ""

        def normalize_img_url(img_url: str) -> str:
            """ç”»åƒURLã‚’æ­£è¦åŒ–ï¼ˆç›¸å¯¾URLã‚’çµ¶å¯¾URLã«å¤‰æ›ï¼‰"""
            img_url = img_url.strip()
            if img_url.startswith('http'):
                return img_url
            elif img_url.startswith('//'):
                return 'https:' + img_url
            elif img_url.startswith('/') and base_url:
                return base_url + img_url
            return ""

        # --- 1. og:image ã‚’è©¦ã¿ã‚‹ ---
        match = re.search(
            r'<meta[^>]+property=["\']og:image["\'][^>]+content=["\'](.*?)["\']',
            html, re.IGNORECASE
        )
        if not match:
            match = re.search(
                r'<meta[^>]+content=["\'](.*?)["\'][^>]+property=["\']og:image["\']',
                html, re.IGNORECASE
            )
        if match:
            img_url = normalize_img_url(match.group(1))
            if img_url and "google.com" not in img_url:
                return img_url

        # --- 2. twitter:image ã‚’è©¦ã¿ã‚‹ ---
        match = re.search(
            r'<meta[^>]+name=["\']twitter:image["\'][^>]+content=["\'](.*?)["\']',
            html, re.IGNORECASE
        )
        if not match:
            match = re.search(
                r'<meta[^>]+content=["\'](.*?)["\'][^>]+name=["\']twitter:image["\']',
                html, re.IGNORECASE
            )
        if match:
            img_url = normalize_img_url(match.group(1))
            if img_url and "google.com" not in img_url:
                return img_url

        # --- 3. è¨˜äº‹æœ¬æ–‡å†…ã®æœ€åˆã®<img>ã‚¿ã‚°ã‚’è©¦ã¿ã‚‹ ---
        # Googleåºƒå‘Šãƒ»ã‚¢ã‚¤ã‚³ãƒ³ãƒ»1pxç”¨ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ãƒ”ã‚¯ã‚»ãƒ«ç­‰ã‚’é™¤å¤–ã™ã‚‹ãŸã‚
        # src ãŒ http(s)ã§å§‹ã¾ã‚Šã€ã‚µã‚¤ã‚ºãŒå°ã•ã™ããªã„ã‚‚ã®ã‚’å„ªå…ˆ
        img_matches = re.findall(
            r'<img[^>]+src=["\']([^"\'<>]+)["\'][^>]*>',
            html, re.IGNORECASE
        )
        for src in img_matches:
            img_url = normalize_img_url(src)
            if not img_url:
                continue
            # é™¤å¤–æ¡ä»¶: ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ãƒ”ã‚¯ã‚»ãƒ«ãƒ»ã‚¢ã‚¤ã‚³ãƒ³ãƒ»googleç³»ã‚’é™¤ã
            lower = img_url.lower()
            if any(skip in lower for skip in ['google', 'gstatic', 'doubleclick', 'adsystem',
                                               'blank', 'spacer', 'pixel', '1x1', 'icon',
                                               'favicon', 'logo', 'avatar', 'gravatar']):
                continue
            # æ‹¡å¼µå­ãƒã‚§ãƒƒã‚¯ï¼ˆç”»åƒã‚‰ã—ã„URLã‚’å„ªå…ˆï¼‰
            if any(lower.endswith(ext) for ext in ['.jpg', '.jpeg', '.png', '.webp', '.gif']):
                return img_url
            # æ‹¡å¼µå­ãŒãªãã¦ã‚‚ç”»åƒãƒ›ã‚¹ãƒ†ã‚£ãƒ³ã‚°ã‚µãƒ¼ãƒ“ã‚¹ã®URLã¯OK
            if any(host in lower for host in ['images.', 'img.', 'cdn.', 'media.', 'assets.',
                                               'photo', 'image', 'pics', 'static']):
                return img_url

    except Exception:
        pass  # ç”»åƒå–å¾—å¤±æ•—ã¯ã‚µã‚¤ãƒ¬ãƒ³ãƒˆã«ã‚¹ã‚­ãƒƒãƒ—
    return ""

def get_random_target():
    """ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰ãƒ©ãƒ³ãƒ€ãƒ ï¼ˆã¾ãŸã¯å¤ã„é †ç­‰ï¼‰ã«åé›†ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚’å–å¾—"""
    conn = database.get_db_connection()
    conn.row_factory = sqlite3.Row
    c = conn.cursor()
    c.execute("SELECT name_ja FROM anime_targets WHERE enabled=1 ORDER BY RANDOM() LIMIT 1")
    row = c.fetchone()
    conn.close()
    return row["name_ja"] if row else None

def process_target(title: str):
    """æŒ‡å®šã•ã‚ŒãŸã‚¿ã‚¤ãƒˆãƒ«ã§æ¤œç´¢ã—ã€ãƒ•ã‚£ãƒ«ã‚¿ï¼†DBä¿å­˜ã‚’è¡Œã†"""
    print(f"\n[Crawler] ğŸ” å¯¾è±¡: {title}")
    
    # æ¤œç´¢ã‚¯ã‚¨ãƒªæ§‹ç¯‰: ã‚¿ã‚¤ãƒˆãƒ«ã‚’å«ã¿ã¤ã¤ã€ã‚°ãƒƒã‚ºãƒ»ã‚³ãƒ©ãƒœãƒ»ã‚¢ãƒ‹ãƒ¡ãªã©ã®ã„ãšã‚Œã‹ãŒå…¥ã£ã¦ã„ã‚‹è¨˜äº‹ã‚’æ¢ã™
    # Google Newsã§ã¯ "AND" ã¯ä¸è¦ï¼ˆã‚¹ãƒšãƒ¼ã‚¹ã§ANDæ‰±ã„ã•ã‚Œã‚‹ï¼‰ã€‚ã¾ãŸåºƒãæ‹¾ã†ãŸã‚ã«ã€Œã‚¢ãƒ‹ãƒ¡ã€ã€Œãƒ•ã‚£ã‚®ãƒ¥ã‚¢ã€ã‚‚è¿½åŠ ã€‚
    search_query = f'"{title}" (ã‚°ãƒƒã‚º OR ã‚³ãƒ©ãƒœ OR ä¸€ç•ªãã˜ OR ã‚«ãƒ•ã‚§ OR ãƒãƒƒãƒ—ã‚¢ãƒƒãƒ— OR äºˆç´„ OR ã‚¢ãƒ‹ãƒ¡ OR ãƒ•ã‚£ã‚®ãƒ¥ã‚¢)'
    
    raw_items = fetch_google_news(search_query)
    print(f"   -> RSSçµæœ: {len(raw_items)} ä»¶")
    
    if not raw_items:
        return
        
    filtered = goods_filter.filter_items(raw_items)
    print(f"   -> ãƒ•ã‚£ãƒ«ã‚¿é€šé: {len(filtered)} ä»¶")
    
    from scorer import score_item
    
    saved = 0
    for item in filtered:
        # ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
        scored_item = score_item(dict(item))
        # DBä¿å­˜
        if database.insert_item(scored_item):
            saved += 1
            # æ–°è¦ä¿å­˜æ™‚ï¼šãŠæ°—ã«å…¥ã‚Šãƒ¦ãƒ¼ã‚¶ãƒ¼ã¸é€šçŸ¥ãƒ•ãƒƒã‚¯ã‚’ç™ºç«
            database.notify_favorited_users(title, scored_item)
            
    print(f"   -> DBæ–°è¦ä¿å­˜: {saved} ä»¶")

def run_crawler():
    print("="*60)
    print(" ğŸš€ ç„¡é™ã‚µãƒ¼ãƒï¼ˆå¸¸é§ã‚¯ãƒ­ãƒ¼ãƒ©ï¼‰èµ·å‹•")
    print("="*60)
    
    while True:
        try:
            # 1. ã¾ãšå„ªå…ˆæ¤œç´¢ã‚­ãƒ¥ãƒ¼ã‚’ãƒã‚§ãƒƒã‚¯
            queued_query = database.get_next_from_queue()
            if queued_query:
                print(f"\n[Queue Priority] ğŸš¨ ãƒ¦ãƒ¼ã‚¶ãƒ¼æ¤œç´¢: {queued_query}")
                process_target(queued_query)
                database.mark_queue_done(queued_query)
            else:
                # 2. ã‚­ãƒ¥ãƒ¼ãŒç©ºãªã‚‰æ—¢å­˜ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‹ã‚‰ãƒ©ãƒ³ãƒ€ãƒ ã§å·¡å›
                target = get_random_target()
                if target:
                    process_target(target)
                else:
                    print("[Crawler] ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãŒã„ã¾ã›ã‚“ã€‚10ç§’å¾…æ©Ÿ...")
            
            # APIã‚„RSSã®ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’é¿ã‘ã‚‹ãŸã‚ã‚¹ãƒªãƒ¼ãƒ—ï¼ˆã‚­ãƒ¥ãƒ¼å‡¦ç†å¾Œã¯å°‘ã—çŸ­ã‚ï¼‰
            time.sleep(8 if queued_query else 15)
            
        except KeyboardInterrupt:
            print("\n[Crawler] çµ‚äº†ã—ã¾ã™ã€‚")
            break
        except Exception as e:
            print(f"\n[Crawler Exception] {e}")
            time.sleep(30)

if __name__ == "__main__":
    # goods_infoã®DBã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
    database.init_db()
    
    # Pythonå®Ÿè¡Œæ™‚ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼å›é¿
    sys.stdout.reconfigure(encoding='utf-8')
    run_crawler()
